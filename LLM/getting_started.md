# Embracing Local Large Language Model Deployment

This marks an exciting new chapter in my journey to deploy large language models (LLMs) locally, eliminating reliance on online services. Like many aspects of life, the value lies not just in achieving the goal but in the transformative experience along the way.

When it comes to running LLMs locally, several tools are available. One prominent option is **Ollama** ([ollama.com](https://ollama.com/)), with straightforward installation guidelines. To optimize its performance, I plan to use open-webUI alongside Ollama, complementing it with Docker as per the official documentation.

Another notable tool is LM Studio, which offers an intuitive user interface and practical chat capabilities. However, my exploration reveals that this platform lacks access to many cutting-edge models, limiting its scope for advanced applications.

Fortunately, there are some excellent open-source LLM models available on Hugging Face, such as:
- [DeepCoder-14B-Preview](https://huggingface.co/agentica-org/DeepCoder-14B-Preview)

These models provide a solid foundation to build upon.